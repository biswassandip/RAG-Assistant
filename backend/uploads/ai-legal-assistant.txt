brew install python@3.11
python3 --version
echo 'export PATH="/opt/homebrew/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc
python3 --version

---
mkdir backend frontend data models logs configs scripts
backend: FastAPI backend (API, LLM integration, RAG)
frontend: React UI (Search, Chatbot, LLM selection)
data: Stores legal documents for indexing & retrieval
models: Local models (Llama, GPT, fine-tuned models)
logs: Stores logs for API requests & debugging
configs: Config files (API keys, LLM settings, DB connections)
scripts: Helper scripts for preprocessing, indexing
---
touch backend/main.py
touch frontend/README.md
touch configs/config.yaml
touch scripts/preprocess.py
touch data/sample_legal_docs.txt
touch .env .gitignore README.md

backend/main.py ‚Üí FastAPI entry point
configs/config.yaml ‚Üí Stores API keys, DB settings
scripts/preprocess.py ‚Üí Preprocesses legal documents
.env ‚Üí Secret keys (LLM API, DB creds)
.gitignore ‚Üí Ignore unwanted files (e.g., venv, __pycache__, .env)
---

ai-legal-assistant/
‚îÇ‚îÄ‚îÄ backend/           # FastAPI Backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py        # Main entry file
‚îÇ   ‚îú‚îÄ‚îÄ routes/        # API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ services/      # Business logic (LLM, RAG)
‚îÇ   ‚îú‚îÄ‚îÄ models/        # Data models & schemas
‚îÇ   ‚îú‚îÄ‚îÄ utils/         # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt  # Dependencies
‚îÇ‚îÄ‚îÄ frontend/          # React Frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/           # React code
‚îÇ   ‚îú‚îÄ‚îÄ public/        # Static assets
‚îÇ   ‚îú‚îÄ‚îÄ package.json   # React dependencies
‚îÇ‚îÄ‚îÄ data/              # Stores legal documents
‚îÇ‚îÄ‚îÄ models/            # LLM models (Llama, etc.)
‚îÇ‚îÄ‚îÄ logs/              # Logs for debugging
‚îÇ‚îÄ‚îÄ configs/           # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml    # Stores API keys, model settings
‚îÇ‚îÄ‚îÄ scripts/           # Helper scripts
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py  # Preprocess legal docs
‚îÇ‚îÄ‚îÄ .gitignore         # Ignore unnecessary files
‚îÇ‚îÄ‚îÄ .env               # Secret environment variables
‚îÇ‚îÄ‚îÄ README.md          # Project Documentation

---

cd ~/path-to-your-repo/ai-legal-assistant/backend
python3 -m venv rag-backend
source venv/bin/activate  # Mac/Linux
pip install fastapi uvicorn langchain transformers pydantic python-dotenv
pip freeze > requirements.txt
deactivate
source rag-backend/bin/activate

---
Set Up FastAPI in /backend
cd ~/path-to-your-repo/ai-legal-assistant/backend
source venv/bin/activate   # Activate virtual environment
touch main.py
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "AI Legal Assistant API is running üöÄ"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run('main:app', host="127.0.0.1", port=8000, reload=True)

---

Organize the Backend Structure
mkdir routes services models utils
touch routes/legal.py services/legal_service.py models/legal_model.py utils/config.py
backend/
‚îÇ‚îÄ‚îÄ main.py               # FastAPI entry point
‚îÇ‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ legal.py          # API routes for legal queries
‚îÇ‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ legal_service.py  # Business logic for processing legal queries
‚îÇ‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ legal_model.py    # Data models for legal documents
‚îÇ‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Configuration settings
‚îÇ‚îÄ‚îÄ venv/                 # Python virtual environment
‚îÇ‚îÄ‚îÄ requirements.txt      # Installed dependencies
‚îÇ‚îÄ‚îÄ .gitignore            # Ignore unnecessary files

---

Add API Endpoints

Add Legal Document Search API (routes/legal.py)
from fastapi import APIRouter

router = APIRouter()

@router.get("/search")
def search_legal_docs(query: str):
    return {"query": query, "results": ["Case 1", "Case 2"]}

---
Register the Route in main.py

from fastapi import FastAPI
from routes import legal

app = FastAPI()

# Register routes
app.include_router(legal.router, prefix="/api")

@app.get("/")
def read_root():
    return {"message": "AI Legal Assistant API is running üöÄ"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run('main:app', host="127.0.0.1", port=8000, reload=True)

---
Test the API
python main.py
Test http://127.0.0.1:8000/api/search?query=contract law
‚úÖ Expected response:

json
Copy
Edit
{
  "query": "contract law",
  "results": ["Case 1", "Case 2"]
}

---
Run Uvicorn Manually
Instead of running python main.py, try running Uvicorn directly:

sh
Copy
Edit
uvicorn main:app --host 127.0.0.1 --port 8000 --reload
‚úÖ If successful, you should see:

pgsql
Copy
Edit
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
---

High-Level Overview:
LLM Setup: We'll integrate a local LLM model like Llama (running in Docker), or configure it to switch between models like GPT-2/3 based on your requirement.
FastAPI Integration: The FastAPI backend will use the selected model for generating legal answers.
Configurable LLM: Allow users to choose the model dynamically via API or configuration settings.
Let‚Äôs proceed step by step.

üìå Step 1: Set Up Local LLM (e.g., Llama)
For local use, Llama (or GPT-2/3) models can be run using Hugging Face or Docker. For simplicity, let's start with Hugging Face transformers that can run the models locally. This will also allow us to switch models easily.

1Ô∏è‚É£ Install Hugging Face Transformers:
Since you‚Äôre working with FastAPI, you‚Äôll need to install the transformers library.

Run the following command inside your activated virtual environment (venv):

sh
Copy
Edit
pip install transformers torch
2Ô∏è‚É£ Set Up Llama or GPT Models in Code:
Now, let‚Äôs integrate the Llama model, or GPT-2 (you can switch to other models like GPT-3 once you get access to them).

Create a file called services/legal_service.py to handle LLM processing. We'll define a function that will use Hugging Face's transformers library to load and use these models.

python
Copy
Edit
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

class LegalModel:
    def __init__(self, model_name="huggingface/llama-13B"):  # default to Llama
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
        self.generator = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer)

    def generate_answer(self, prompt: str) -> str:
        response = self.generator(prompt, max_length=150, num_return_sequences=1)
        return response[0]["generated_text"]
Explanation:
LegalModel class loads a model based on the provided name (huggingface/llama-13B is just an example; you can change it to any model name like GPT-2).
The generate_answer function uses the model to generate answers to a legal query.
üìå Step 2: FastAPI Endpoint for Legal Questions
Now, let‚Äôs add an API endpoint to FastAPI that will accept legal questions and return answers using the LLM model.

In routes/legal.py, add:

python
Copy
Edit
from fastapi import APIRouter
from services.legal_service import LegalModel

router = APIRouter()

# Initialize model
legal_model = LegalModel()

@router.get("/legal-answer")
def get_legal_answer(query: str):
    response = legal_model.generate_answer(query)
    return {"query": query, "answer": response}
Explanation:
We define an endpoint /legal-answer that accepts a query string.
The query is passed to the LegalModel instance, which processes it and returns a generated response.
üìå Step 3: Configuring LLM via Environment Variables (Optional)
We‚Äôll make the LLM model configurable via an environment variable, so you can easily switch between models (like GPT-2, Llama, etc.) without changing code.

Create a .env file inside the backend/ folder and add:

ini
Copy
Edit
LLM_MODEL=huggingface/llama-13B  # You can change this to any model you want (e.g., GPT-2)
Then, update services/legal_service.py to read from the .env file using the python-dotenv library.

1Ô∏è‚É£ Install python-dotenv:

sh
Copy
Edit
pip install python-dotenv
2Ô∏è‚É£ Update legal_service.py to load the model name from .env:

python
Copy
Edit
import os
from dotenv import load_dotenv
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Load environment variables
load_dotenv()

class LegalModel:
    def __init__(self):
        model_name = os.getenv("LLM_MODEL", "huggingface/llama-13B")  # Default to Llama if not set
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.generator = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer)

    def generate_answer(self, prompt: str) -> str:
        response = self.generator(prompt, max_length=150, num_return_sequences=1)
        return response[0]["generated_text"]
üìå Step 4: Running the Application
1Ô∏è‚É£ Activate Virtual Environment:

sh
Copy
Edit
cd ~/path-to-your-repo/ai-legal-assistant/backend
source venv/bin/activate
2Ô∏è‚É£ Start FastAPI server:

sh
Copy
Edit
uvicorn main:app --host 127.0.0.1 --port 8000 --reload
üìå Step 5: Testing the Legal Query API
Once your server is running, you can test it via:

Test API Call in Browser or Postman:
nginx
Copy
Edit
GET http://127.0.0.1:8000/api/legal-answer?query=What is intellectual property?
You should receive a response like:

json
Copy
Edit
{
    "query": "What is intellectual property?",
    "answer": "Intellectual property (IP) refers to legal rights that are granted to individuals or organizations over their inventions, creations, and designs."
}
üìå Step 6: Optional - Switch Between Models Dynamically
If you want to switch models dynamically based on the query or user selection, you can modify your API to accept a model query parameter:

python
Copy
Edit
@router.get("/legal-answer")
def get_legal_answer(query: str, model: str = "huggingface/llama-13B"):
    legal_model = LegalModel(model_name=model)  # Use the selected model
    response = legal_model.generate_answer(query)
    return {"query": query, "answer": response}
Now, you can pass the model as part of the URL like:

nginx
Copy
Edit
GET http://127.0.0.1:8000/api/legal-answer?query=What is intellectual property?&model=gpt2
‚úÖ Summary
You now have a locally running LLM-powered legal AI assistant integrated with FastAPI, and the ability to switch between models. üéâ

Next Steps
Integrate Retrieval-Augmented Generation (RAG) for real-time document search.
Add a frontend (React) to interact with the API and display answers.
Fine-tune the model for legal-specific answers (if required).
Let me know which step you‚Äôd like to proceed with next! üöÄ


---

Option 1: Using SQLite (Local Database)
SQLite is a lightweight, serverless database that is easy to set up and suitable for local development.

1Ô∏è‚É£ Install SQLAlchemy (ORM for Python)
First, install SQLAlchemy and SQLite dependencies:

sh
Copy
Edit
pip install SQLAlchemy
pip install sqlite (should already exist in Mac)
2Ô∏è‚É£ Define the Database Model
Now, create a file to define the database schema. In the backend/ folder, create a new file called models.py.

python
Copy
Edit
from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import datetime

Base = declarative_base()

class LegalDocument(Base):
    __tablename__ = 'legal_documents'

    id = Column(Integer, primary_key=True)
    title = Column(String, nullable=False)
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.datetime.utcnow)
    
    def __repr__(self):
        return f"LegalDocument(id={self.id}, title={self.title}, created_at={self.created_at})"
Explanation:
LegalDocument represents a table with legal documents that includes the title, content, and a timestamp (created_at).
We are using SQLAlchemy ORM to create this table.
3Ô∏è‚É£ Set Up Database Connection
Next, in your backend/ folder, create a new file called database.py to handle the database connection.

python
Copy
Edit
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from models import Base

DATABASE_URL = "sqlite:///./legal_documents.db"  # SQLite local database file

# Create database engine
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})

# Create session maker
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create the tables in the database
Base.metadata.create_all(bind=engine)
4Ô∏è‚É£ Create Database and Session
Now, you need a way to interact with the database in your FastAPI routes. You‚Äôll use SQLAlchemy session to read/write data.

In routes/legal.py (or create a new file like routes/database_operations.py), you‚Äôll add the following logic:

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from pydantic import BaseModel  # For request body validation
from models import LegalDocument
from database import SessionLocal

router = APIRouter()

# Dependency to get the database session
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Pydantic model to validate incoming request body
class LegalDocumentCreate(BaseModel):
    title: str
    content: str

# API route to get all legal documents
@router.get("/legal-documents")
def get_legal_documents(db: Session = Depends(get_db)):
    documents = db.query(LegalDocument).all()
    return documents

# API route to add a legal document
@router.post("/legal-documents")
def add_legal_document(
    legal_document: LegalDocumentCreate, db: Session = Depends(get_db)
):
    db_document = LegalDocument(
        title=legal_document.title, content=legal_document.content
    )
    db.add(db_document)
    db.commit()
    db.refresh(db_document)
    return db_document

Explanation:
get_db is a FastAPI dependency that manages the database session.
/legal-documents endpoint retrieves all documents stored in the database.
POST /legal-documents allows you to add legal documents to the database.
5Ô∏è‚É£ Test the Database Integration
To test that everything is working, you can run the FastAPI backend:

sh
Copy
Edit
uvicorn main:app --host 127.0.0.1 --port 8000 --reload
Open your browser or Postman to test the routes.
Get all documents: GET http://127.0.0.1:8000/legal-documents
Add a document: POST http://127.0.0.1:8000/legal-documents with JSON body:
json
Copy
Edit
{
  "title": "Intellectual Property Law",
  "content": "This document explains intellectual property law in detail."
}
You should be able to see the documents stored in your local SQLite database.

Option 2: Using PostgreSQL (Production-Grade Database)
If you want a more robust database with better scalability, you can use PostgreSQL.

1Ô∏è‚É£ Install PostgreSQL
First, you need to install PostgreSQL on your machine. If you're using Mac, you can use Homebrew:

sh
Copy
Edit
brew install postgresql
Start the PostgreSQL service:

sh
Copy
Edit
brew services start postgresql
2Ô∏è‚É£ Update Database URL
Change the database URL in database.py to use PostgreSQL instead of SQLite.

python
Copy
Edit
DATABASE_URL = "postgresql://username:password@localhost/dbname"
For local PostgreSQL:

Replace username with your PostgreSQL username (default is postgres).
Replace password with your PostgreSQL password.
Replace dbname with the name of your database.
You can create a database using:

sh
Copy
Edit
psql postgres
CREATE DATABASE legal_documents;
3Ô∏è‚É£ Set Up PostgreSQL Session
The rest of the code in database.py remains mostly the same. Just ensure you have the PostgreSQL driver installed:

sh
Copy
Edit
pip install psycopg2-binary
‚úÖ Summary for Database Setup:
SQLite (Local Database):
Simple, fast, and serverless, perfect for small or local applications.
Easy to configure and get started with.
PostgreSQL (Production-Grade):
Scalable, and can handle complex queries and larger datasets.
You can use it for production-level applications.
---

