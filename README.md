# ğŸš€ RAG-Backend: Local LLM-Powered Retrieval-Augmented Generation System

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-Enabled-green)


---

## ğŸ† Introduction: What is RAG-Backend?
**RAG-Backend** is a powerful **open-source, local LLM-powered Retrieval-Augmented Generation (RAG) system** designed for **intelligent document-based answering**. It allows users to **upload multiple file types** (PDFs, DOCX, Excel, XML, etc.), **index them into a vector database**, and **perform intelligent searches** to get **accurate and context-aware responses**.  

### ğŸ¯ Key Highlights:
- **Works Offline** â€“ No internet dependency, **privacy-first** approach.
- **Multi-format Support** â€“ Handles **PDFs, Word Docs, Excel, XML, etc.**
- **Hybrid Search Mechanism** â€“ Combines **vector-based retrieval (FAISS)** + **keyword-based retrieval (BM25)**.
- **Local AI Chatbot** â€“ Uses **Llama2, Mistral-7B, TinyLlama** via `llama.cpp` for **smart, real-time answers**.
- **Optimized for Speed & Accuracy** â€“ Uses **LangChain** for **query expansion & efficient search ranking**.

---

## ğŸ¤” What is RAG? (Retrieval-Augmented Generation)
Imagine you're a **student preparing for exams**.  
You have tons of PDFs, notes, research papers, and books on your laptop.  
You want to **quickly find answers** instead of searching manually.  

**Traditional Search:**  
ğŸ” Searching for "**Newtonâ€™s Third Law**" in your PDFs **only finds matching keywords** but **doesnâ€™t explain** the concept.  

**With RAG:**  
âœ… You **ask your AI assistant**: *"Explain Newton's Third Law in simple terms with examples."*  
âœ… The AI **retrieves the most relevant documents** related to Newtonâ€™s Laws.  
âœ… The AI **understands** the content and **generates a summarized answer** in **simple language**.  
âœ… The AI can **cite the exact sources** where the information came from.  

ğŸ”¹ **Result:** You get an **accurate, AI-generated explanation** **without manually reading all documents!**  

### ğŸ›  How RAG Works (Technical Breakdown)
**Retrieval-Augmented Generation (RAG)** combines **two steps**:

1ï¸âƒ£ **Retrieval Phase**:  
   - The system **scans the uploaded documents** and **finds the most relevant content** based on the userâ€™s query.  
   - Uses **FAISS** (dense vector retrieval) for **semantic similarity search**.  
   - Uses **BM25** (sparse retrieval) for **keyword-based matching**.  
   - Uses **LangChain** to **expand queries & rank results efficiently**.  

2ï¸âƒ£ **Generation Phase**:  
   - The **retrieved content is passed** to a **local LLM (e.g., Llama2, Mistral-7B)**.  
   - The model **understands the context** and **generates a well-structured response**.  
   - The answer is **streamed in real-time** so the user gets an instant reply.  

---

## ğŸ— **RAG-Backend System Architecture**
```mermaid
graph TD;
    subgraph User Interaction
        User -->|Uploads Files| FileProcessor;
        User -->|Asks Question| QueryProcessor;
    end

    subgraph Document Processing
        FileProcessor -->|Extracts text| Chunking;
        Chunking -->|Embeds| FAISSIndex;
        FAISSIndex -->|Stores & Retrieves| VectorDB;
    end

    subgraph Query Handling
        QueryProcessor -->|Retrieves Relevant Docs| VectorDB;
        QueryProcessor -->|Expands Query| LangChain;
        LangChain -->|Passes Context| LocalLLM;
        LocalLLM -->|Generates Response| ResponseStreamer;
        ResponseStreamer -->|Streams Answer| User;
    end

```
---

## To use, download the below LLMs and tryout by updating the config giving the Local LLM Path
### Larger Models

#### Llama 2 GGUF
```
wget -P models https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf
```

#### Mistral 7B GGUF
```
wget -P https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
```

### Smaller Models
#### TinyLlama 1.1B (FASTEST)
```
wget -P https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF/resolve/main/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf
```

#### Mistral 7B (Quantized)
```
wget -P https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
```

#### Llama 2 7B (Quantized)
```
wget -P https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf
```

---

## ğŸ”¥ Key Features

### âœ… 1. Multi-format Document Processing
- Parses and indexes **PDFs, DOCX, Excel, XML, Text**.
- Uses **OCR (Tesseract)** for images and **XML parsers** for structured files.

### âœ… 2. Hybrid Search
- **FAISS-based vector search** for **semantic matching**.
- **BM25 keyword-based search** for **keyword matching**.

### âœ… 3. Local LLM Integration
- Works with **TinyLlama, Llama2, Mistral-7B** models via `llama.cpp`.

### âœ… 4. Streaming AI Chat (WebSockets)
- **No delays** â€“ responses are **streamed token-by-token** in real-time.

### âœ… 5. Efficient Query Expansion (LangChain)
- Enhances queries using **synonym-based expansion**.
- Improves **retrieval accuracy** via **reranking & hybrid search**.

---

## ğŸ›  Tech Stack

### **Programming Language:**
ğŸŸ¢ **Python 3.10++**

### **Key Libraries & Frameworks:**
- ğŸ— **LangChain** â€“ RAG and Query Optimization
- ğŸ“– **FAISS** â€“ Dense Vector Search
- ğŸ† **BM25** â€“ Sparse Text Search
- ğŸ–¼ **Tesseract OCR** â€“ Image Text Extraction
- ğŸ§  **Llama.cpp** â€“ Local LLM Inference
- ğŸ–‹ **Sentence Transformers** â€“ Hugging Face Embeddings

---

## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/rag-backend.git
cd rag-backend

### 2ï¸âƒ£ Create a Virtual Environment
```bash
Copy
Edit
python -m venv venv
source venv/bin/activate   # On macOS/Linux
venv\Scripts\activate      # On Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
Copy
Edit
pip install -r requirements.txt
```

####  Download the LLM (see above for links) and copy to /models

### 4ï¸âƒ£ Start the Backend
```bash
Copy
Edit
python main.py
```

### 5ï¸âƒ£ Access the Web Interface
Open http://localhost:8000 in your browser.

####  Go to config and update the Local LLM path
---

## ğŸ¯ Future Enhancements
- âœ… Support for Audio & Video Transcription (e.g., Whisper AI)
- âœ… Fine-tuned LLMs for better contextual responses
- âœ… Support for additional vector databases (Chroma, Pinecone, Weaviate)
- âœ… Parallel processing for large-scale document indexing

---
